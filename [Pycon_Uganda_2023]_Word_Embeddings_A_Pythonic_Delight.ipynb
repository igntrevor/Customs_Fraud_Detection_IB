{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/igntrevor/Customs_Fraud_Detection_IB/blob/master/%5BPycon_Uganda_2023%5D_Word_Embeddings_A_Pythonic_Delight.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Pycon Uganda 2023] Word Embeddings: A Pythonic Delight\n",
        "\n",
        "## Introduction\n",
        "Today, we have chatbots all over the internet that have simplified and automated processes like sales, onboarding, etc., for thousands of organizations.<br>\n",
        "We have artificial intelligenceâ€“based question answering systems that are beating benchmark scores set by human beings.<br>\n",
        "We have deep learning models that are capable of generating entire essays that are indistinguishable from human-written ones.<br>\n",
        "All these tasks require large amounts of training data, but how do algorithms and machine learning models understand human-written text data? The answer is ***vectors***.\n",
        "\n",
        "Vectors are the fundamental entity in the field of linear algebra and are responsible for rapid development in the field of natural language processing (NLP) in the last few decades. Any document, sentence, or even a word in a given dataset is represented as a unique vector, and its configuration is decided by the other vectors in the dataset (vocabulary).\n",
        "\n",
        "## Natural Language Processing\n",
        "According to [Wikipedia](https://en.wikipedia.org/wiki/Natural_language_processing), natural language processing is \"a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\" NLP is a vast field with many tasks, features, and utilites, but we are only going to focus on different processes used to encode text data and process it.\n",
        "\n",
        "But we know it all starts from Data Right?"
      ],
      "metadata": {
        "id": "eNwVWDfec6WN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Cleaning"
      ],
      "metadata": {
        "id": "pKGWYAjGiFpp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read Data\n",
        "Starting from a Python interpreter we first need to import the urllib.request module with:"
      ],
      "metadata": {
        "id": "Qaft8-_dibj5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RH-8XYrHcy9O"
      },
      "outputs": [],
      "source": [
        "import urllib.request"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use urllib.request to download the text for a famous book, Moby Dick, from the Gutenberg project with:"
      ],
      "metadata": {
        "id": "YsdkFNnKirpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://www.gutenberg.org/files/2701/2701-0.txt\"\n",
        "file = urllib.request.urlopen(url)"
      ],
      "metadata": {
        "id": "T4XkYjDriu7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we decode and combine the text with a list comprehension and join like so:"
      ],
      "metadata": {
        "id": "_4cYKtLSi7wK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = [line.decode('utf-8') for line in file]\n",
        "text = ''.join(text)"
      ],
      "metadata": {
        "id": "S2IrLQBui-Ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets now print out some of the text:"
      ],
      "metadata": {
        "id": "1bXAIqTMjLPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text[7600:8000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "2isQljixjPcC",
        "outputId": "a243d968-c121-447c-a3a9-f441e2a62f31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'poor devil of a Sub-Sub appears to have gone through the long\\r\\n  Vaticans and street-stalls of the earth, picking up whatever random\\r\\n  allusions to whales he could anyways find in any book whatsoever,\\r\\n  sacred or profane. Therefore you must not, in every case at least,\\r\\n  take the higgledy-piggledy whale statements, however authentic, in\\r\\n  these extracts, for veritable gospel cetology. Far from'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenize\n",
        "Moby Dick is now completely captured in text. Next we need to tokenize or break the document into words.\n",
        "\n",
        "We can do that easily with:"
      ],
      "metadata": {
        "id": "KF8bwOamjbjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = text.split()"
      ],
      "metadata": {
        "id": "7XWQHDbejfPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The split function splits the text on whitespace. We can display a range of words with:"
      ],
      "metadata": {
        "id": "rBCQoYvvjnA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens[200:222]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_zD_NqUjo53",
        "outputId": "618d9736-fd9b-4af7-c790-7eac35836fe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['13.',\n",
              " 'Wheelbarrow.',\n",
              " 'CHAPTER',\n",
              " '14.',\n",
              " 'Nantucket.',\n",
              " 'CHAPTER',\n",
              " '15.',\n",
              " 'Chowder.',\n",
              " 'CHAPTER',\n",
              " '16.',\n",
              " 'The',\n",
              " 'Ship.',\n",
              " 'CHAPTER',\n",
              " '17.',\n",
              " 'The',\n",
              " 'Ramadan.',\n",
              " 'CHAPTER',\n",
              " '18.',\n",
              " 'His',\n",
              " 'Mark.',\n",
              " 'CHAPTER',\n",
              " '19.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We often want to ignore capitalization of words. So we will first lowercase all the text and then tokenize like so:"
      ],
      "metadata": {
        "id": "tkLMnwI_jyHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = text.lower().split()\n",
        "tokens[200:222]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eczD5_ibjzRt",
        "outputId": "5a368219-fda9-41b1-e938-431252dd23cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['13.',\n",
              " 'wheelbarrow.',\n",
              " 'chapter',\n",
              " '14.',\n",
              " 'nantucket.',\n",
              " 'chapter',\n",
              " '15.',\n",
              " 'chowder.',\n",
              " 'chapter',\n",
              " '16.',\n",
              " 'the',\n",
              " 'ship.',\n",
              " 'chapter',\n",
              " '17.',\n",
              " 'the',\n",
              " 'ramadan.',\n",
              " 'chapter',\n",
              " '18.',\n",
              " 'his',\n",
              " 'mark.',\n",
              " 'chapter',\n",
              " '19.']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove Punctuation\n",
        "You may have noticed that many of our tokens still have punctuation. For basic NLP tasks we will often want to isolate or remove punctuation.\n",
        "\n",
        "We can remove punctuation by first making a translation table with:"
      ],
      "metadata": {
        "id": "L9gwxJRLkL1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "table = str.maketrans('', '', string.punctuation)"
      ],
      "metadata": {
        "id": "Xh10jdF_kT2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The translation table will allow us to translate punctuation to empty using:"
      ],
      "metadata": {
        "id": "XzvGGi2jkeXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [w.translate(table) for w in tokens]\n",
        "tokens[200:222]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GXh7bz3kfyr",
        "outputId": "86e5c3e6-d09a-4636-d52b-a4e928caaeba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['13',\n",
              " 'wheelbarrow',\n",
              " 'chapter',\n",
              " '14',\n",
              " 'nantucket',\n",
              " 'chapter',\n",
              " '15',\n",
              " 'chowder',\n",
              " 'chapter',\n",
              " '16',\n",
              " 'the',\n",
              " 'ship',\n",
              " 'chapter',\n",
              " '17',\n",
              " 'the',\n",
              " 'ramadan',\n",
              " 'chapter',\n",
              " '18',\n",
              " 'his',\n",
              " 'mark',\n",
              " 'chapter',\n",
              " '19']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Only Alphabetic\n",
        "For most NLP tasks we want to stick with just language or alphabetic characters. That means removing all non-alphabetic characters.\n",
        "\n",
        "We can remove all numeric characters with:"
      ],
      "metadata": {
        "id": "NmPNt7w6mv50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [word for word in tokens if word.isalpha()]"
      ],
      "metadata": {
        "id": "2Fm3ZXi3m1aC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The if word.isalpha() test filters out all numeric characters.\n",
        "\n",
        "Then we can look at the results with:"
      ],
      "metadata": {
        "id": "X9KSBMkZm6Tr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens[200:222]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsYR0bSLm_K3",
        "outputId": "560a86d0-3753-4782-a62f-747af6dd5ef2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['chapter',\n",
              " 'going',\n",
              " 'aboard',\n",
              " 'chapter',\n",
              " 'merry',\n",
              " 'christmas',\n",
              " 'chapter',\n",
              " 'the',\n",
              " 'lee',\n",
              " 'shore',\n",
              " 'chapter',\n",
              " 'the',\n",
              " 'advocate',\n",
              " 'chapter',\n",
              " 'postscript',\n",
              " 'chapter',\n",
              " 'knights',\n",
              " 'and',\n",
              " 'squires',\n",
              " 'chapter',\n",
              " 'knights',\n",
              " 'and']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What's Next\n",
        "What we have covered above has allowed us to reduce our document to a set of tokens. This will work for most simple applications.\n",
        "\n",
        "For complex applications we would need to reduce or translate the tokens further using the following concepts:\n",
        "\n",
        "- Remove stop words: like 'the'\n",
        "- Stemming of words: like 'like' from:\n",
        "  - likes\n",
        "  - liked\n",
        "  - likely\n",
        "  - liking\n",
        "- Lemmatization of words:\n",
        "  - rocks -> rock\n",
        "  - corpora -> corpus\n",
        "  - better -> good\n",
        "\n",
        "Those tasks are better done with a full featured library like NLTK.\n",
        "\n",
        "Let's Dig Deeper."
      ],
      "metadata": {
        "id": "7_N15qRHnMSf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction To NTLK"
      ],
      "metadata": {
        "id": "_ZeP8D2ip_rh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading Our Data\n",
        "We had already used the urllib.request to download the text for a famous book, Moby Dick, from the Gutenberg project with.\n",
        "\n",
        "Now lets go ahead and print an excerpt with:"
      ],
      "metadata": {
        "id": "0aO_zEgzqNT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text[7600:8000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "MUKbLoIwqR3R",
        "outputId": "b29889c2-5fbc-4cd0-8f3f-2467c9a8c2f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'poor devil of a Sub-Sub appears to have gone through the long\\r\\n  Vaticans and street-stalls of the earth, picking up whatever random\\r\\n  allusions to whales he could anyways find in any book whatsoever,\\r\\n  sacred or profane. Therefore you must not, in every case at least,\\r\\n  take the higgledy-piggledy whale statements, however authentic, in\\r\\n  these extracts, for veritable gospel cetology. Far from'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization\n",
        "With the text document loaded we can move on tokenizing.\n",
        "\n",
        "NLTK comes with a number of modules and corpora for performing NLP and learning about it. For our purposes we only need the standard tokenizer called 'punkt'.\n",
        "\n",
        "We can setup and download 'punkt' with the following code:"
      ],
      "metadata": {
        "id": "EdoionqGrEOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9hNKHoStcCf",
        "outputId": "525cac21-5086-4f47-9903-abddba57bfe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With NLTK we can tokenize the document into sentences with:"
      ],
      "metadata": {
        "id": "MIAlG9U8tof6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import sent_tokenize\n",
        "sentences = sent_tokenize(text)"
      ],
      "metadata": {
        "id": "h6BiH4Fhtruv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences[200]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0jT9HI7-tyPc",
        "outputId": "c285718a-dc93-4600-f27b-0407ec1f8641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Lamp.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or, tokenize the document into words with:"
      ],
      "metadata": {
        "id": "pUBDVAAKt5SK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "tokens = word_tokenize(text)"
      ],
      "metadata": {
        "id": "wi7RTWedt6n5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens[200:222]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmk6JRLyuBP5",
        "outputId": "4c9d02ab-f059-4a87-eeb9-c659a6768e37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.',\n",
              " 'CHAPTER',\n",
              " '3',\n",
              " '.',\n",
              " 'The',\n",
              " 'Spouter-Inn',\n",
              " '.',\n",
              " 'CHAPTER',\n",
              " '4',\n",
              " '.',\n",
              " 'The',\n",
              " 'Counterpane',\n",
              " '.',\n",
              " 'CHAPTER',\n",
              " '5',\n",
              " '.',\n",
              " 'Breakfast',\n",
              " '.',\n",
              " 'CHAPTER',\n",
              " '6',\n",
              " '.',\n",
              " 'The']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clean Text\n",
        "With the document tokenized we can return to cleaning the text.\n",
        "\n",
        "We can remove all numeric characters with:"
      ],
      "metadata": {
        "id": "GAQaQ4FGuoYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [word for word in tokens if word.isalpha()]\n",
        "tokens[200:222]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Cs9nw2fuqwv",
        "outputId": "8ef4a855-563d-44dd-edd3-6e14efba2eef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Shore',\n",
              " 'CHAPTER',\n",
              " 'The',\n",
              " 'Advocate',\n",
              " 'CHAPTER',\n",
              " 'Postscript',\n",
              " 'CHAPTER',\n",
              " 'Knights',\n",
              " 'and',\n",
              " 'Squires',\n",
              " 'CHAPTER',\n",
              " 'Knights',\n",
              " 'and',\n",
              " 'Squires',\n",
              " 'CHAPTER',\n",
              " 'Ahab',\n",
              " 'CHAPTER',\n",
              " 'Enter',\n",
              " 'Ahab',\n",
              " 'to',\n",
              " 'Him',\n",
              " 'Stubb']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove punctuation with:"
      ],
      "metadata": {
        "id": "6NlLzGnUu2-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "tokens = [w.translate(table) for w in tokens]\n",
        "tokens[200:222]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_AgubpPu6eT",
        "outputId": "b4cf5471-162d-4e98-dceb-13bb8cdec03f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Shore',\n",
              " 'CHAPTER',\n",
              " 'The',\n",
              " 'Advocate',\n",
              " 'CHAPTER',\n",
              " 'Postscript',\n",
              " 'CHAPTER',\n",
              " 'Knights',\n",
              " 'and',\n",
              " 'Squires',\n",
              " 'CHAPTER',\n",
              " 'Knights',\n",
              " 'and',\n",
              " 'Squires',\n",
              " 'CHAPTER',\n",
              " 'Ahab',\n",
              " 'CHAPTER',\n",
              " 'Enter',\n",
              " 'Ahab',\n",
              " 'to',\n",
              " 'Him',\n",
              " 'Stubb']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, lowercase everything with:"
      ],
      "metadata": {
        "id": "jf41Ab1GvGtv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [word.lower() for word in tokens]\n",
        "tokens[200:222]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyBLSYlovHnh",
        "outputId": "623eadec-341d-4cd2-8036-b9bb129be944"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['shore',\n",
              " 'chapter',\n",
              " 'the',\n",
              " 'advocate',\n",
              " 'chapter',\n",
              " 'postscript',\n",
              " 'chapter',\n",
              " 'knights',\n",
              " 'and',\n",
              " 'squires',\n",
              " 'chapter',\n",
              " 'knights',\n",
              " 'and',\n",
              " 'squires',\n",
              " 'chapter',\n",
              " 'ahab',\n",
              " 'chapter',\n",
              " 'enter',\n",
              " 'ahab',\n",
              " 'to',\n",
              " 'him',\n",
              " 'stubb']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stop Words\n",
        "Stop words are common words in NLP that often are better filtered out. These words may include: 'you', 'the', 'it' and so on.\n",
        "\n",
        "NLTK provides a preassembled list of stop words we can download like so:"
      ],
      "metadata": {
        "id": "T343LRg8v_Qq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQ7HNKDpwFFT",
        "outputId": "6938225e-155e-46f7-8496-42fe0d8e1e97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we can remove all the stop words from the tokens with:"
      ],
      "metadata": {
        "id": "oySsqP2iwKm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = [w for w in tokens if not w in stop_words]\n",
        "tokens[200:222]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1ssDaMQwNzD",
        "outputId": "6bb0ca5c-10b4-4830-96a1-1833655f347d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['pictures',\n",
              " 'whaling',\n",
              " 'scenes',\n",
              " 'chapter',\n",
              " 'whales',\n",
              " 'paint',\n",
              " 'teeth',\n",
              " 'wood',\n",
              " 'stone',\n",
              " 'mountains',\n",
              " 'stars',\n",
              " 'chapter',\n",
              " 'brit',\n",
              " 'chapter',\n",
              " 'squid',\n",
              " 'chapter',\n",
              " 'line',\n",
              " 'chapter',\n",
              " 'stubb',\n",
              " 'kills',\n",
              " 'whale',\n",
              " 'chapter']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming\n",
        "Stemming is the process of bringing a word to its root value. We clip the stems from the word. Take for example the words like: acquire - acquired, firm - firms or product - production. Reducing a word to its root simplifies the NLP task with little loss to the meaning.\n",
        "\n",
        "Reducing a word to its root value can have many variations. Fortunately, NLTK provides several and we will use PorterStemmer.\n",
        "\n",
        "We can load the PorterStemmer and stem the words easily with:"
      ],
      "metadata": {
        "id": "eeQfn0_vwZKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "stemmed = [porter.stem(word) for word in tokens]\n",
        "stemmed[200:222]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJuqgWjwwiKB",
        "outputId": "6b6beb84-976c-4400-f90a-a4efd12b554d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['pictur',\n",
              " 'whale',\n",
              " 'scene',\n",
              " 'chapter',\n",
              " 'whale',\n",
              " 'paint',\n",
              " 'teeth',\n",
              " 'wood',\n",
              " 'stone',\n",
              " 'mountain',\n",
              " 'star',\n",
              " 'chapter',\n",
              " 'brit',\n",
              " 'chapter',\n",
              " 'squid',\n",
              " 'chapter',\n",
              " 'line',\n",
              " 'chapter',\n",
              " 'stubb',\n",
              " 'kill',\n",
              " 'whale',\n",
              " 'chapter']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With a clean set of tokens in hand we can move on to discovering more about words in the next section"
      ],
      "metadata": {
        "id": "J51zEr6kwqGv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction To Bag Of Words"
      ],
      "metadata": {
        "id": "Hf9J_3P8xUm_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read Data\n",
        "\n",
        "Since our Data is already in the notebook we will proceed with the next steps"
      ],
      "metadata": {
        "id": "q4NdLP9yxY0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenize and Clean\n",
        "After the document is loaded we can proceed to tokenize and clean the document.\n",
        "\n",
        "First we tokenize with:"
      ],
      "metadata": {
        "id": "bsOv0KR1xju7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "tokens = word_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6j4a1RgxxtnO",
        "outputId": "a1d60b8e-6052-4b46-e756-e24c8982585c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then clean with:"
      ],
      "metadata": {
        "id": "XxAKb0Iux0wi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "tokens = [word for word in tokens if word.isalpha()]\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "tokens = [w.translate(table) for w in tokens]\n",
        "tokens = [word.lower() for word in tokens]"
      ],
      "metadata": {
        "id": "K8fuRxu4x3y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, remove stop words and stem with:"
      ],
      "metadata": {
        "id": "ecRXS-zRx_H-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = [w for w in tokens if not w in stop_words]\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "tokens = [porter.stem(word) for word in tokens]\n",
        "tokens[200:222]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zC4sPzg8x_0u",
        "outputId": "538f5882-3ae8-41a8-f729-e65e389dd46c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['pictur',\n",
              " 'whale',\n",
              " 'scene',\n",
              " 'chapter',\n",
              " 'whale',\n",
              " 'paint',\n",
              " 'teeth',\n",
              " 'wood',\n",
              " 'stone',\n",
              " 'mountain',\n",
              " 'star',\n",
              " 'chapter',\n",
              " 'brit',\n",
              " 'chapter',\n",
              " 'squid',\n",
              " 'chapter',\n",
              " 'line',\n",
              " 'chapter',\n",
              " 'stubb',\n",
              " 'kill',\n",
              " 'whale',\n",
              " 'chapter']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vocabulary\n",
        "With a clean set of tokens we can move on to understanding the vocabulary. A vocabulary of a document represents all the words in that document and the frequency they appear.\n",
        "\n",
        "NLTK has the FreqDist class that can help us count the words in a document with:"
      ],
      "metadata": {
        "id": "BSuMtrJ2yKOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "\n",
        "word_counts = FreqDist(tokens)\n",
        "word_counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55eDHLP3yjMM",
        "outputId": "b59347a8-b63f-4525-d121-be7353cfd632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'whale': 1454, 'one': 920, 'like': 590, 'upon': 567, 'ship': 553, 'ye': 521, 'man': 496, 'ahab': 495, 'sea': 461, 'seem': 460, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that all the words are counted we can extract a vocabulary. In many cases we only want to understand the most frequent words. We can take the top most frequent words like so:"
      ],
      "metadata": {
        "id": "A-xIK3MkytAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top = 500\n",
        "vocabulary = word_counts.most_common(top)\n",
        "\n",
        "vocabulary[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pp2miDcywZ7",
        "outputId": "0c063f7b-591f-4fa4-d2cc-707a52856d72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('whale', 1454),\n",
              " ('one', 920),\n",
              " ('like', 590),\n",
              " ('upon', 567),\n",
              " ('ship', 553),\n",
              " ('ye', 521),\n",
              " ('man', 496),\n",
              " ('ahab', 495),\n",
              " ('sea', 461),\n",
              " ('seem', 460)]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For most NLP tasks we may want a larger vocabulary that uses 5000 words or more."
      ],
      "metadata": {
        "id": "JwgVqcqIyu0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Count Vector\n",
        "With a vocabulary established we can move on to scoring words. A simple way to score words is by frequency. We can then combine the frequency scores of words with the vocabulary and create a count vector or bag of words.\n",
        "\n",
        "First we will import a helper library called numpy. Numpy helps us store all manners of data for machine learning."
      ],
      "metadata": {
        "id": "hpEiCcFpJT9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "xESl91nvzw9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we can create a word vector of the words and counts like so:"
      ],
      "metadata": {
        "id": "cdGDKQd6z1nD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "voc_size = len(vocabulary)\n",
        "doc_vector = np.zeros(voc_size)\n",
        "\n",
        "word_vector = [(idx,word_counts[word[0]]) for idx, word in enumerate(vocabulary) if word[0] in word_counts.keys()]\n",
        "word_vector[10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nM4sOg1dz5NG",
        "outputId": "8236e2e5-0ce2-42b0-da48-ac8417a74321"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 443)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we can create a count vector for the document with:"
      ],
      "metadata": {
        "id": "nTi3JAUv0D6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, count in word_vector:\n",
        "  doc_vector[idx] = count\n",
        "\n",
        "doc_vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylIPdeO80FK9",
        "outputId": "d3b7edac-65a5-44dd-c4de-4f4aaff17343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1454.,  920.,  590.,  567.,  553.,  521.,  496.,  495.,  461.,\n",
              "        460.,  443.,  434.,  429.,  424.,  365.,  342.,  338.,  337.,\n",
              "        331.,  322.,  317.,  315.,  312.,  312.,  311.,  308.,  307.,\n",
              "        298.,  292.,  284.,  280.,  277.,  277.,  277.,  268.,  268.,\n",
              "        266.,  256.,  255.,  251.,  249.,  247.,  243.,  241.,  240.,\n",
              "        238.,  236.,  231.,  230.,  228.,  224.,  222.,  217.,  217.,\n",
              "        215.,  211.,  211.,  205.,  204.,  204.,  203.,  203.,  201.,\n",
              "        196.,  196.,  193.,  192.,  191.,  190.,  189.,  184.,  182.,\n",
              "        182.,  180.,  179.,  178.,  176.,  175.,  171.,  171.,  168.,\n",
              "        168.,  168.,  167.,  167.,  164.,  161.,  159.,  159.,  159.,\n",
              "        153.,  153.,  153.,  152.,  148.,  143.,  142.,  140.,  139.,\n",
              "        138.,  137.,  134.,  132.,  132.,  130.,  129.,  129.,  128.,\n",
              "        128.,  128.,  127.,  126.,  126.,  125.,  125.,  124.,  123.,\n",
              "        122.,  122.,  122.,  121.,  121.,  121.,  120.,  119.,  119.,\n",
              "        119.,  118.,  118.,  118.,  115.,  115.,  114.,  113.,  113.,\n",
              "        113.,  112.,  112.,  110.,  110.,  108.,  107.,  106.,  106.,\n",
              "        105.,  105.,  105.,  105.,  104.,  104.,  103.,  103.,  102.,\n",
              "        102.,  100.,   99.,   98.,   98.,   97.,   97.,   97.,   96.,\n",
              "         96.,   94.,   94.,   94.,   93.,   93.,   93.,   93.,   92.,\n",
              "         91.,   91.,   91.,   91.,   90.,   90.,   90.,   90.,   90.,\n",
              "         89.,   89.,   88.,   88.,   88.,   87.,   87.,   87.,   87.,\n",
              "         87.,   87.,   86.,   86.,   86.,   84.,   84.,   83.,   83.,\n",
              "         83.,   82.,   82.,   81.,   81.,   80.,   80.,   80.,   80.,\n",
              "         80.,   80.,   80.,   80.,   80.,   79.,   79.,   79.,   78.,\n",
              "         78.,   78.,   78.,   78.,   78.,   77.,   77.,   77.,   77.,\n",
              "         76.,   76.,   76.,   76.,   76.,   76.,   75.,   75.,   75.,\n",
              "         75.,   75.,   75.,   75.,   75.,   75.,   75.,   74.,   74.,\n",
              "         74.,   74.,   73.,   72.,   72.,   72.,   72.,   71.,   71.,\n",
              "         71.,   70.,   70.,   69.,   69.,   69.,   68.,   68.,   68.,\n",
              "         68.,   68.,   68.,   68.,   67.,   67.,   67.,   67.,   67.,\n",
              "         66.,   66.,   65.,   65.,   65.,   64.,   64.,   64.,   64.,\n",
              "         64.,   64.,   63.,   63.,   63.,   63.,   63.,   62.,   62.,\n",
              "         62.,   61.,   61.,   61.,   60.,   60.,   60.,   60.,   60.,\n",
              "         60.,   60.,   60.,   59.,   59.,   59.,   59.,   59.,   58.,\n",
              "         57.,   57.,   57.,   57.,   56.,   56.,   56.,   56.,   56.,\n",
              "         56.,   56.,   56.,   56.,   56.,   56.,   56.,   56.,   55.,\n",
              "         55.,   55.,   55.,   55.,   55.,   54.,   54.,   54.,   54.,\n",
              "         54.,   54.,   54.,   54.,   54.,   54.,   53.,   53.,   53.,\n",
              "         53.,   53.,   53.,   53.,   53.,   52.,   52.,   52.,   52.,\n",
              "         52.,   52.,   52.,   51.,   51.,   51.,   51.,   51.,   51.,\n",
              "         51.,   51.,   51.,   50.,   50.,   50.,   50.,   50.,   50.,\n",
              "         50.,   50.,   50.,   50.,   50.,   49.,   49.,   49.,   49.,\n",
              "         49.,   49.,   48.,   48.,   48.,   48.,   48.,   48.,   48.,\n",
              "         48.,   47.,   47.,   47.,   47.,   47.,   47.,   47.,   47.,\n",
              "         47.,   47.,   47.,   47.,   46.,   46.,   46.,   46.,   46.,\n",
              "         46.,   45.,   45.,   45.,   45.,   45.,   45.,   45.,   45.,\n",
              "         45.,   45.,   45.,   45.,   45.,   45.,   45.,   44.,   44.,\n",
              "         44.,   44.,   44.,   44.,   44.,   44.,   44.,   44.,   44.,\n",
              "         43.,   43.,   43.,   43.,   43.,   43.,   43.,   43.,   43.,\n",
              "         43.,   43.,   43.,   43.,   43.,   43.,   43.,   43.,   42.,\n",
              "         42.,   42.,   42.,   42.,   42.,   42.,   42.,   42.,   42.,\n",
              "         42.,   42.,   42.,   42.,   42.,   42.,   42.,   41.,   41.,\n",
              "         41.,   41.,   41.,   41.,   41.,   41.,   41.,   41.,   41.,\n",
              "         41.,   41.,   41.,   41.,   41.,   41.,   40.,   40.,   40.,\n",
              "         40.,   40.,   40.,   40.,   40.,   40.,   40.,   40.,   40.,\n",
              "         40.,   40.,   39.,   39.,   39.])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bag of Words\n",
        "We now have all the pieces to make a bag of words model for a set of documents.\n",
        "\n",
        "For documents we will use a small collection of sentences from the Moby Dick text like so:"
      ],
      "metadata": {
        "id": "FzhOEGxz0doP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import sent_tokenize\n",
        "\n",
        "docs = sent_tokenize(text)[703:706]\n",
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0w_6Q4N_0hUA",
        "outputId": "68c63b21-953e-4490-88b5-ea32e07cbb7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The more I pondered over this harpooneer, the more I abominated the\\r\\nthought of sleeping with him.',\n",
              " 'It was fair to presume that being a\\r\\nharpooneer, his linen or woollen, as the case might be, would not be of\\r\\nthe tidiest, certainly none of the finest.',\n",
              " 'I began to twitch all over.']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we want to import a helper module from SciKitLearn called CountVectorizer with:"
      ],
      "metadata": {
        "id": "rNSGqgy10pYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "kT4oGyft0o1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The helper CountVectorizer can tokenize, clean, and count all the tokens in our documents with:"
      ],
      "metadata": {
        "id": "KZjjPBC40yhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_vectorizer=CountVectorizer(stop_words='english')\n",
        "\n",
        "word_count_vector=count_vectorizer.fit_transform(docs)\n",
        "word_count_vector.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJCaNfNq0zVC",
        "outputId": "f318da55-a4ce-4f05-848d-e9831cd541d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 15)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The shape of the word_count_vector represents the number of documents (3) and total number of words (15) in those documents.\n",
        "\n",
        "The word_count_vector represents the documents bag of words. We can view the contents of this by:"
      ],
      "metadata": {
        "id": "P_aAlw6O09M2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_count_vector.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13hULuw41A1a",
        "outputId": "7c1ea635-98c3-4e5e-adfc-ac53ca347390"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0],\n",
              "       [0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1],\n",
              "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Where each 1D vector represents positions for the entire vocabulary. Each value of 1 represents that word is present in the document or in this case sentence.\n",
        "\n",
        "We can view that list of words by querying the count_tokenizer with:"
      ],
      "metadata": {
        "id": "lMkh0tWO1LMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_vectorizer.get_feature_names_out()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEy7eQXeLtLd",
        "outputId": "1b18fa29-0b6b-4971-c00f-09a007f73e73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['abominated', 'began', 'case', 'certainly', 'fair', 'finest',\n",
              "       'harpooneer', 'linen', 'pondered', 'presume', 'sleeping',\n",
              "       'thought', 'tidiest', 'twitch', 'woollen'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's Now Talk Word Embeddings"
      ],
      "metadata": {
        "id": "qNKnM6u16p4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read Data\n",
        "For this lab we are going to use a nursery rhyme as a set of sample or toy documents. Using toy documents will allow us to better understand how similarity works with documents.\n",
        "\n",
        "We will use the first 4 verses from the nursery rhyme \"The House that Jack Built\" (1755 London):"
      ],
      "metadata": {
        "id": "f2bDmlUF61BP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [\"This is the house that Jack built. \"\n",
        "        \"This is the cheese that lay in the house that Jack built. \"\n",
        "        \"This is the rat that ate the cheese \"\n",
        "        \"That lay in the house that Jack built. \"\n",
        "        \"This is the cat that chased the rat \"\n",
        "        \"That ate the cheese that lay in the house that Jack built. \",  #verse 1\n",
        "        \"This is the dog that worried the cat \"\n",
        "        \"That chased the rat that ate the cheese \"\n",
        "        \"That lay in the house that Jack built. \"\n",
        "        \"This is the cow with the crumpled horn \"\n",
        "        \"That tossed the dog that worried the cat \"\n",
        "        \"That chased the rat that ate the cheese \"\n",
        "        \"That lay in the house that Jack built. \", # verse 2\n",
        "        \"This is the maiden all forlorn \"\n",
        "        \"That milked the cow with the crumpled horn \"\n",
        "        \"That tossed the dog that worried the cat \"\n",
        "        \"That chased the rat that ate the cheese \"\n",
        "        \"That lay in the house that Jack built. \", # verse 3\n",
        "        \"This is the man all tattered and torn \"\n",
        "        \"That kissed the maiden all forlorn \"\n",
        "        \"That milked the cow with the crumpled horn \"\n",
        "        \"That tossed the dog that worried the cat \"\n",
        "        \"That chased the rat that ate the cheese \"\n",
        "        \"That lay in the house that Jack built. \" # verse 4\n",
        "        ]"
      ],
      "metadata": {
        "id": "wUVNlqXt7fn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that each verse is a document separated by a comma ','.\n",
        "\n",
        "Then, to make sure everything is loaded correctly we output docs with:"
      ],
      "metadata": {
        "id": "q9FYADN07nIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Zfl0k1V7o-Z",
        "outputId": "34669c5e-394c-48a9-db91-474417232143"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This is the house that Jack built. This is the cheese that lay in the house that Jack built. This is the rat that ate the cheese That lay in the house that Jack built. This is the cat that chased the rat That ate the cheese that lay in the house that Jack built. ',\n",
              " 'This is the dog that worried the cat That chased the rat that ate the cheese That lay in the house that Jack built. This is the cow with the crumpled horn That tossed the dog that worried the cat That chased the rat that ate the cheese That lay in the house that Jack built. ',\n",
              " 'This is the maiden all forlorn That milked the cow with the crumpled horn That tossed the dog that worried the cat That chased the rat that ate the cheese That lay in the house that Jack built. ',\n",
              " 'This is the man all tattered and torn That kissed the maiden all forlorn That milked the cow with the crumpled horn That tossed the dog that worried the cat That chased the rat that ate the cheese That lay in the house that Jack built. ']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bag of Words\n",
        "With the documents loaded our first step is to construct a bag of words. We can do this using CountVectorizer from SciKit Learn.\n",
        "\n",
        "First we load the vectorizer and then instantiate it with:"
      ],
      "metadata": {
        "id": "9pnkxLNd7yqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vectorizer=CountVectorizer(stop_words='english')"
      ],
      "metadata": {
        "id": "ygPzCMh07x2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The we can construct the bag of words with:"
      ],
      "metadata": {
        "id": "EGo7psER77yn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bag_of_words=count_vectorizer.fit_transform(docs)\n",
        "bag_of_words.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Hj_8ng_78xr",
        "outputId": "6ede45a2-d0c8-4269-c016-246455d644ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 22)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We display the words in the vocabulary with:"
      ],
      "metadata": {
        "id": "TJi9RsYe8IKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_vectorizer.get_feature_names_out()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1vQlTLMQBPg",
        "outputId": "fb0af0c5-5d29-4448-c52b-75f0c911b8c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['ate', 'built', 'cat', 'chased', 'cheese', 'cow', 'crumpled',\n",
              "       'dog', 'forlorn', 'horn', 'house', 'jack', 'kissed', 'lay',\n",
              "       'maiden', 'man', 'milked', 'rat', 'tattered', 'torn', 'tossed',\n",
              "       'worried'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then look at each document vector with"
      ],
      "metadata": {
        "id": "eu5rtB0G_XjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bag_of_words.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqiXXXg8_ZiP",
        "outputId": "b66ba0e3-24fe-4280-8b40-294b0238afb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2, 4, 1, 1, 3, 0, 0, 0, 0, 0, 4, 4, 0, 3, 0, 0, 0, 2, 0, 0, 0, 0],\n",
              "       [2, 2, 2, 2, 2, 1, 1, 2, 0, 1, 2, 2, 0, 2, 0, 0, 0, 2, 0, 0, 1, 2],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF\n",
        "Scoring documents by the frequency of words ignores uncommon words that may contain more relevant meaning.\n",
        "\n",
        "A better method of scoring documents is called Term Frequency over Inverse Document Frequency or TF-IDF. This score takes the frequency of a word in a document and divides it by inverse frequency of that word as it appears in all the documents.\n",
        "\n",
        "Fortunately, SciKitLearn has a transformer that can take the bag of words(count) vector and transform it to a TF-IDF vector.\n",
        "\n",
        "First let us import the TfidfTransformer with:"
      ],
      "metadata": {
        "id": "e3WSuQ7a_ksX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer"
      ],
      "metadata": {
        "id": "0ZO6Oudd_o9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we can calculate the TF-IDF scores using:"
      ],
      "metadata": {
        "id": "OuZr0lz5_wQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
        "tfidf_transformer.fit(bag_of_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "dCTpla2z_xUP",
        "outputId": "d229b597-5f34-4254-ea42-e67cea20e3ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfTransformer()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfTransformer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The TfidfTransformer converts the word counts to TF-IDF scores.\n",
        "\n",
        "Finally, we can review the scores using pandas and the following:"
      ],
      "metadata": {
        "id": "1p352olX__VB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "# Your previously calculated bag_of_words\n",
        "# (Assuming you have already run the code to calculate bag_of_words)\n",
        "# bag_of_words = count_vectorizer.fit_transform(docs)\n",
        "\n",
        "# Create a TfidfTransformer\n",
        "tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
        "\n",
        "# Fit the transformer to the bag_of_words and transform it to TF-IDF scores\n",
        "tfidf_scores = tfidf_transformer.fit_transform(bag_of_words)\n",
        "\n",
        "# Convert the TF-IDF scores to a pandas DataFrame\n",
        "tfidf_df = pd.DataFrame(tfidf_scores.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Print or view the TF-IDF scores\n",
        "print(tfidf_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QUalekmCb-3",
        "outputId": "dbaa7643-88e7-498a-8170-a43d69bd55cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        ate     built       cat    chased    cheese       cow  crumpled  \\\n",
            "0  0.229416  0.458831  0.114708  0.114708  0.344124  0.000000  0.000000   \n",
            "1  0.272284  0.272284  0.272284  0.272284  0.272284  0.166521  0.166521   \n",
            "2  0.200707  0.200707  0.200707  0.200707  0.200707  0.245493  0.245493   \n",
            "3  0.159085  0.159085  0.159085  0.159085  0.159085  0.194584  0.194584   \n",
            "\n",
            "        dog   forlorn      horn  ...    kissed       lay    maiden       man  \\\n",
            "0  0.000000  0.000000  0.000000  ...  0.000000  0.344124  0.000000  0.000000   \n",
            "1  0.333043  0.000000  0.166521  ...  0.000000  0.272284  0.000000  0.000000   \n",
            "2  0.245493  0.303233  0.245493  ...  0.000000  0.200707  0.303233  0.000000   \n",
            "3  0.194584  0.240350  0.194584  ...  0.304854  0.159085  0.240350  0.304854   \n",
            "\n",
            "     milked       rat  tattered      torn    tossed   worried  \n",
            "0  0.000000  0.229416  0.000000  0.000000  0.000000  0.000000  \n",
            "1  0.000000  0.272284  0.000000  0.000000  0.166521  0.333043  \n",
            "2  0.303233  0.200707  0.000000  0.000000  0.245493  0.245493  \n",
            "3  0.240350  0.159085  0.304854  0.304854  0.194584  0.194584  \n",
            "\n",
            "[4 rows x 22 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at the scores and notice the more frequent terms like 'ate' and 'jack' have a TF-IDF score of 1.0. While less frequent words like 'torn' and 'kissed' have a higher score."
      ],
      "metadata": {
        "id": "F7GuHfXxDcu2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cosine Similarity (Distance)\n",
        "In order to measure the similarity between 2 documents or in this case verses, we can measure the distances in TF-IDF document vectors.\n",
        "\n",
        "There are a number of methods we may use to measure distance between any n dimensional vectors. The most common method is called cosine distance. We calculate the cosine distance of any 2 vectors (v1 and v2) by:\n",
        "\n",
        "distance = dot product(v1,v2) / (len(v1) * len(v2))\n",
        "\n",
        "Distance calculated will range from 1.0, almost exact, to -1.0, the exact opposite, no matter what vectors we use.\n",
        "\n",
        "Let's calculate the distance between the verses. First we will import TfidfVectorizer to create TF-IDF vectors from our document in one step, like so:"
      ],
      "metadata": {
        "id": "qlaxUpyzHgUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "tfidf = vectorizer.fit_transform(docs)"
      ],
      "metadata": {
        "id": "ikQIsbDnHkKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we can quickly measure the similarity with:"
      ],
      "metadata": {
        "id": "qlOKVI09HjpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pairwise_similarity = tfidf * tfidf.T\n",
        "pairwise_similarity.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWoImS2IHtqV",
        "outputId": "6cd47461-e068-401f-e570-d9c91a62156b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 0.7495952 , 0.55254323, 0.43796031],\n",
              "       [0.7495952 , 1.        , 0.81888181, 0.64906728],\n",
              "       [0.55254323, 0.81888181, 1.        , 0.79262632],\n",
              "       [0.43796031, 0.64906728, 0.79262632, 1.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can format that view better with:"
      ],
      "metadata": {
        "id": "r9JnO0McH4lc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "terms = pd.DataFrame(pairwise_similarity.toarray())\n",
        "terms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "eOeojYrkH2Bz",
        "outputId": "b54dc7f0-7099-4832-ab63-1d27c82e05cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0         1         2         3\n",
              "0  1.000000  0.749595  0.552543  0.437960\n",
              "1  0.749595  1.000000  0.818882  0.649067\n",
              "2  0.552543  0.818882  1.000000  0.792626\n",
              "3  0.437960  0.649067  0.792626  1.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2d296c7e-bf66-4985-80ec-1558a1f49ae8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.749595</td>\n",
              "      <td>0.552543</td>\n",
              "      <td>0.437960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.749595</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.818882</td>\n",
              "      <td>0.649067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.552543</td>\n",
              "      <td>0.818882</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.792626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.437960</td>\n",
              "      <td>0.649067</td>\n",
              "      <td>0.792626</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2d296c7e-bf66-4985-80ec-1558a1f49ae8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2d296c7e-bf66-4985-80ec-1558a1f49ae8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2d296c7e-bf66-4985-80ec-1558a1f49ae8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e3491dcb-bc88-4356-bfe8-530153aeb1cb\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e3491dcb-bc88-4356-bfe8-530153aeb1cb')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e3491dcb-bc88-4356-bfe8-530153aeb1cb button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The diagonal represents documents comparing to itself, hence the 1.0. Notice how different the 1st and 4th document are."
      ],
      "metadata": {
        "id": "4Mrt5O-yH9KE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings\n",
        "We can also measure word similarity in a similar manner by using a concept called word embeddings. Word embeddings are vector representations of words learned from either a statistical or deep learning model.\n",
        "\n",
        "The most common embedding model is Word2Vec from a library called gensim. Document lists of words are fed into the model and the model learns a vector representation of those words. We can then use that vector representation to measure similarity between words in our corpus.\n",
        "\n",
        "First, we want to tokenize the documents into sentences and list of words with:"
      ],
      "metadata": {
        "id": "aQBdZjmjIJ02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "doc = '.'.join(docs)\n",
        "data = []\n",
        "for sent in sent_tokenize(doc):\n",
        "  temp = []\n",
        "  for word in word_tokenize(sent):\n",
        "    temp.append(word)\n",
        "  data.append(temp)\n",
        "\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_e_w4e98INE3",
        "outputId": "45c67452-80f4-4423-e267-8815887412f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['This', 'is', 'the', 'house', 'that', 'Jack', 'built', '.'],\n",
              " ['This',\n",
              "  'is',\n",
              "  'the',\n",
              "  'cheese',\n",
              "  'that',\n",
              "  'lay',\n",
              "  'in',\n",
              "  'the',\n",
              "  'house',\n",
              "  'that',\n",
              "  'Jack',\n",
              "  'built',\n",
              "  '.'],\n",
              " ['This',\n",
              "  'is',\n",
              "  'the',\n",
              "  'rat',\n",
              "  'that',\n",
              "  'ate',\n",
              "  'the',\n",
              "  'cheese',\n",
              "  'That',\n",
              "  'lay',\n",
              "  'in',\n",
              "  'the',\n",
              "  'house',\n",
              "  'that',\n",
              "  'Jack',\n",
              "  'built',\n",
              "  '.'],\n",
              " ['This',\n",
              "  'is',\n",
              "  'the',\n",
              "  'cat',\n",
              "  'that',\n",
              "  'chased',\n",
              "  'the',\n",
              "  'rat',\n",
              "  'That',\n",
              "  'ate',\n",
              "  'the',\n",
              "  'cheese',\n",
              "  'that',\n",
              "  'lay',\n",
              "  'in',\n",
              "  'the',\n",
              "  'house',\n",
              "  'that',\n",
              "  'Jack',\n",
              "  'built',\n",
              "  '.'],\n",
              " ['.This',\n",
              "  'is',\n",
              "  'the',\n",
              "  'dog',\n",
              "  'that',\n",
              "  'worried',\n",
              "  'the',\n",
              "  'cat',\n",
              "  'That',\n",
              "  'chased',\n",
              "  'the',\n",
              "  'rat',\n",
              "  'that',\n",
              "  'ate',\n",
              "  'the',\n",
              "  'cheese',\n",
              "  'That',\n",
              "  'lay',\n",
              "  'in',\n",
              "  'the',\n",
              "  'house',\n",
              "  'that',\n",
              "  'Jack',\n",
              "  'built',\n",
              "  '.'],\n",
              " ['This',\n",
              "  'is',\n",
              "  'the',\n",
              "  'cow',\n",
              "  'with',\n",
              "  'the',\n",
              "  'crumpled',\n",
              "  'horn',\n",
              "  'That',\n",
              "  'tossed',\n",
              "  'the',\n",
              "  'dog',\n",
              "  'that',\n",
              "  'worried',\n",
              "  'the',\n",
              "  'cat',\n",
              "  'That',\n",
              "  'chased',\n",
              "  'the',\n",
              "  'rat',\n",
              "  'that',\n",
              "  'ate',\n",
              "  'the',\n",
              "  'cheese',\n",
              "  'That',\n",
              "  'lay',\n",
              "  'in',\n",
              "  'the',\n",
              "  'house',\n",
              "  'that',\n",
              "  'Jack',\n",
              "  'built',\n",
              "  '.'],\n",
              " ['.This',\n",
              "  'is',\n",
              "  'the',\n",
              "  'maiden',\n",
              "  'all',\n",
              "  'forlorn',\n",
              "  'That',\n",
              "  'milked',\n",
              "  'the',\n",
              "  'cow',\n",
              "  'with',\n",
              "  'the',\n",
              "  'crumpled',\n",
              "  'horn',\n",
              "  'That',\n",
              "  'tossed',\n",
              "  'the',\n",
              "  'dog',\n",
              "  'that',\n",
              "  'worried',\n",
              "  'the',\n",
              "  'cat',\n",
              "  'That',\n",
              "  'chased',\n",
              "  'the',\n",
              "  'rat',\n",
              "  'that',\n",
              "  'ate',\n",
              "  'the',\n",
              "  'cheese',\n",
              "  'That',\n",
              "  'lay',\n",
              "  'in',\n",
              "  'the',\n",
              "  'house',\n",
              "  'that',\n",
              "  'Jack',\n",
              "  'built',\n",
              "  '.'],\n",
              " ['.This',\n",
              "  'is',\n",
              "  'the',\n",
              "  'man',\n",
              "  'all',\n",
              "  'tattered',\n",
              "  'and',\n",
              "  'torn',\n",
              "  'That',\n",
              "  'kissed',\n",
              "  'the',\n",
              "  'maiden',\n",
              "  'all',\n",
              "  'forlorn',\n",
              "  'That',\n",
              "  'milked',\n",
              "  'the',\n",
              "  'cow',\n",
              "  'with',\n",
              "  'the',\n",
              "  'crumpled',\n",
              "  'horn',\n",
              "  'That',\n",
              "  'tossed',\n",
              "  'the',\n",
              "  'dog',\n",
              "  'that',\n",
              "  'worried',\n",
              "  'the',\n",
              "  'cat',\n",
              "  'That',\n",
              "  'chased',\n",
              "  'the',\n",
              "  'rat',\n",
              "  'that',\n",
              "  'ate',\n",
              "  'the',\n",
              "  'cheese',\n",
              "  'That',\n",
              "  'lay',\n",
              "  'in',\n",
              "  'the',\n",
              "  'house',\n",
              "  'that',\n",
              "  'Jack',\n",
              "  'built',\n",
              "  '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will import the Word2Vec model and create it with:"
      ],
      "metadata": {
        "id": "ZsNWo8LMJEY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "model1 = gensim.models.Word2Vec(data, min_count = 1, vector_size = 10, window = 5)"
      ],
      "metadata": {
        "id": "FrO3zMqbJFmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can look at the model's vocabulary with:"
      ],
      "metadata": {
        "id": "GS8rC5AxJhcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1.wv.key_to_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y60kxQJDJig0",
        "outputId": "add39aa8-876b-4ec5-bfd1-487ba9316c86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 0,\n",
              " 'that': 1,\n",
              " 'That': 2,\n",
              " 'is': 3,\n",
              " 'house': 4,\n",
              " 'Jack': 5,\n",
              " 'built': 6,\n",
              " '.': 7,\n",
              " 'cheese': 8,\n",
              " 'lay': 9,\n",
              " 'in': 10,\n",
              " 'ate': 11,\n",
              " 'rat': 12,\n",
              " 'This': 13,\n",
              " 'cat': 14,\n",
              " 'chased': 15,\n",
              " 'worried': 16,\n",
              " 'dog': 17,\n",
              " '.This': 18,\n",
              " 'cow': 19,\n",
              " 'with': 20,\n",
              " 'crumpled': 21,\n",
              " 'horn': 22,\n",
              " 'tossed': 23,\n",
              " 'all': 24,\n",
              " 'maiden': 25,\n",
              " 'forlorn': 26,\n",
              " 'milked': 27,\n",
              " 'torn': 28,\n",
              " 'man': 29,\n",
              " 'tattered': 30,\n",
              " 'and': 31,\n",
              " 'kissed': 32}"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then measure the similarity between 2 words with:\n",
        "\n"
      ],
      "metadata": {
        "id": "tlBR5jk0JvY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1.wv.similarity('Jack', 'rat')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fl8DbPRMKHiG",
        "outputId": "872908ff-77ee-4111-c8f5-2ef659b78381"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.05713532"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model we created here is called a Continuous Bag of Words or CBOW. You should know there are other variations to embeddings models."
      ],
      "metadata": {
        "id": "Q-_9q6fFKMOf"
      }
    }
  ]
}